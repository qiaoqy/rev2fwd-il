# å›¾åƒå½’ä¸€åŒ–åˆ†æï¼šè®­ç»ƒ vs æ¨ç†

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æäº†åœ¨è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ä¸­ï¼Œå›¾åƒæ•°æ®çš„å½’ä¸€åŒ–å’Œåå½’ä¸€åŒ–å¤„ç†æµç¨‹ã€‚

## ç›®å½•
1. [è®­ç»ƒæ—¶çš„å›¾åƒå¤„ç†](#è®­ç»ƒæ—¶çš„å›¾åƒå¤„ç†)
2. [æ¨ç†æ—¶çš„å›¾åƒå¤„ç†](#æ¨ç†æ—¶çš„å›¾åƒå¤„ç†)
3. [ä¸ºä»€ä¹ˆXYZå¯è§†åŒ–ä¸­å›¾åƒç»è¿‡å½’ä¸€åŒ–](#ä¸ºä»€ä¹ˆxyzå¯è§†åŒ–ä¸­å›¾åƒç»è¿‡å½’ä¸€åŒ–)
4. [Lossè®¡ç®—ä¸­æ˜¯å¦åŒ…å«å›¾åƒLoss](#lossè®¡ç®—ä¸­æ˜¯å¦åŒ…å«å›¾åƒloss)
5. [æ€»ç»“](#æ€»ç»“)

---

## è®­ç»ƒæ—¶çš„å›¾åƒå¤„ç†

### 1. æ•°æ®è½¬æ¢é˜¶æ®µ (NPZ â†’ LeRobot Dataset)

**ä½ç½®**: `scripts/31_train_A_diffusion.py` â†’ `convert_npz_to_lerobot_format()`

```python
# åŸå§‹å›¾åƒ: uint8 [0, 255], shape (H, W, 3)
img = images[t]  # (H, W, 3) uint8

frame = {
    "observation.image": img,  # ç›´æ¥å­˜å‚¨ uint8 å›¾åƒ
    "observation.state": state.astype(np.float32),
    "action": action.astype(np.float32),
}

dataset.add_frame(frame)  # LeRobotä¼šå°†å›¾åƒç¼–ç ä¸ºè§†é¢‘
```

**å…³é”®ç‚¹**:
- åŸå§‹å›¾åƒä»¥ **uint8 [0, 255]** æ ¼å¼å­˜å‚¨
- LeRobot Dataset å°†å›¾åƒç¼–ç ä¸º **è§†é¢‘æ–‡ä»¶** (MP4)
- æ­¤æ—¶ **æ²¡æœ‰è¿›è¡Œå½’ä¸€åŒ–**

### 2. è®­ç»ƒæ•°æ®åŠ è½½é˜¶æ®µ

**ä½ç½®**: LeRobot å†…éƒ¨çš„ `LeRobotDataset.__getitem__()`

å½“ä» LeRobot Dataset åŠ è½½æ•°æ®æ—¶:

```python
# LeRobot å†…éƒ¨å¤„ç† (ä¼ªä»£ç )
# 1. ä»è§†é¢‘æ–‡ä»¶è§£ç å›¾åƒ
image = decode_video_frame(...)  # è¿”å› uint8 [0, 255]

# 2. è½¬æ¢ä¸º float32 å¹¶å½’ä¸€åŒ–åˆ° [0, 1]
image = image.astype(np.float32) / 255.0  # [0, 1]

# 3. è½¬æ¢ä¸º CHW æ ¼å¼
image = np.transpose(image, (2, 0, 1))  # (H, W, 3) -> (3, H, W)

# 4. è½¬æ¢ä¸º Tensor
image = torch.from_numpy(image)  # (3, H, W) float32 [0, 1]
```

**å…³é”®ç‚¹**:
- LeRobot Dataset åœ¨åŠ è½½æ—¶ **è‡ªåŠ¨å°†å›¾åƒå½’ä¸€åŒ–åˆ° [0, 1]**
- æ ¼å¼è½¬æ¢: `(H, W, 3) uint8 [0, 255]` â†’ `(3, H, W) float32 [0, 1]`

### 3. Preprocessor å¤„ç†

**ä½ç½®**: `src/rev2fwd_il/train/lerobot_train_with_viz.py` â†’ `train_with_xyz_visualization()`

```python
# åˆ›å»º preprocessor æ—¶é…ç½®å½’ä¸€åŒ–
preprocessor_overrides = {
    "normalizer_processor": {
        "stats": dataset.meta.stats,
        "features": {**policy.config.input_features, **policy.config.output_features},
        "norm_map": policy.config.normalization_mapping,
    },
}

preprocessor, postprocessor = make_pre_post_processors(
    policy_cfg=cfg.policy,
    pretrained_path=cfg.policy.pretrained_path,
    preprocessor_overrides=preprocessor_overrides,
    ...
)
```

**Normalization Mapping** (ä»è®­ç»ƒæ—¥å¿—):
```
[DEBUG] TRAINING Normalization Settings
  policy_cfg.normalization_mapping:
    FeatureType.STATE: NormalizationMode.MEAN_STD
    FeatureType.ACTION: NormalizationMode.MEAN_STD
    FeatureType.VISUAL: NormalizationMode.NONE  # å›¾åƒä¸å†å½’ä¸€åŒ–!
```

**å…³é”®ç‚¹**:
- **å›¾åƒ (VISUAL) çš„å½’ä¸€åŒ–æ¨¡å¼æ˜¯ `NONE`**
- è¿™æ˜¯å› ä¸ºå›¾åƒå·²ç»åœ¨ Dataset åŠ è½½æ—¶å½’ä¸€åŒ–åˆ° [0, 1]
- Preprocessor **ä¸ä¼šå¯¹å›¾åƒè¿›è¡Œé¢å¤–çš„å½’ä¸€åŒ–**
- åªå¯¹ `observation.state` å’Œ `action` è¿›è¡Œ mean-std å½’ä¸€åŒ–

### 4. è®­ç»ƒæ—¶çš„å®Œæ•´æµç¨‹

```python
# 1. Dataset åŠ è½½ (LeRobot å†…éƒ¨)
batch = dataset[idx]
# batch["observation.image"]: (B, n_obs_steps, 3, H, W) float32 [0, 1]
# batch["observation.state"]: (B, n_obs_steps, state_dim) float32 (åŸå§‹å€¼)
# batch["action"]: (B, horizon, action_dim) float32 (åŸå§‹å€¼)

# 2. Preprocessor å½’ä¸€åŒ–
processed_batch = preprocessor(batch)
# processed_batch["observation.image"]: (B, n_obs_steps, 3, H, W) float32 [0, 1] (ä¸å˜)
# processed_batch["observation.state"]: (B, n_obs_steps, state_dim) float32 (å½’ä¸€åŒ–å)
# processed_batch["action"]: (B, horizon, action_dim) float32 (å½’ä¸€åŒ–å)

# 3. Policy forward (è®¡ç®— loss)
loss, output_dict = policy.forward(processed_batch)
# loss åªè®¡ç®— action çš„ MSE lossï¼Œä¸åŒ…å«å›¾åƒ loss
```

---

## æ¨ç†æ—¶çš„å›¾åƒå¤„ç†

### 1. ä»ç¯å¢ƒè·å–å›¾åƒ

**ä½ç½®**: `scripts/41_test_A_diffusion_visualize.py` â†’ `run_episode()`

```python
# ä» Isaac Lab ç›¸æœºè·å–å›¾åƒ
table_rgb = table_camera.data.output["rgb"]  # (num_envs, H, W, 4) uint8
if table_rgb.shape[-1] > 3:
    table_rgb = table_rgb[..., :3]  # å»æ‰ alpha é€šé“
table_rgb_np = table_rgb.cpu().numpy().astype(np.uint8)  # (1, H, W, 3) uint8
table_rgb_frame = table_rgb_np[0]  # (H, W, 3) uint8 [0, 255]

# è½¬æ¢ä¸º float32 [0, 1] å¹¶è½¬ä¸º BCHW æ ¼å¼
table_rgb_chw = torch.from_numpy(table_rgb_frame).float() / 255.0  # uint8 -> float [0,1]
table_rgb_chw = table_rgb_chw.permute(2, 0, 1).unsqueeze(0).to(device)  # (1, 3, H, W)
```

**å…³é”®ç‚¹**:
- ä»ç›¸æœºè·å–çš„æ˜¯ **uint8 [0, 255]** å›¾åƒ
- **æ‰‹åŠ¨å½’ä¸€åŒ–åˆ° [0, 1]**: `/ 255.0`
- æ ¼å¼è½¬æ¢: `(H, W, 3)` â†’ `(1, 3, H, W)`

### 2. Preprocessor å¤„ç†

```python
policy_inputs = {
    "observation.image": table_rgb_chw,  # (1, 3, H, W) float32 [0, 1]
    "observation.state": state,  # (1, state_dim) float32 (åŸå§‹å€¼)
}

# Preprocessor å½’ä¸€åŒ–
if preprocessor is not None:
    policy_inputs = preprocessor(policy_inputs)
# policy_inputs["observation.image"]: (1, 3, H, W) float32 [0, 1] (ä¸å˜)
# policy_inputs["observation.state"]: (1, state_dim) float32 (å½’ä¸€åŒ–å)
```

**å…³é”®ç‚¹**:
- æ¨ç†æ—¶çš„ preprocessor é…ç½®ä¸è®­ç»ƒæ—¶ **å®Œå…¨ç›¸åŒ**
- å›¾åƒå½’ä¸€åŒ–æ¨¡å¼ä»ç„¶æ˜¯ `NONE`
- å›¾åƒä¿æŒåœ¨ [0, 1] èŒƒå›´ï¼Œ**ä¸ä¼šè¿›è¡Œé¢å¤–å½’ä¸€åŒ–**

### 3. æ¨ç†æ—¶çš„å®Œæ•´æµç¨‹

```python
# 1. è·å–åŸå§‹å›¾åƒå¹¶å½’ä¸€åŒ–
table_rgb_chw = torch.from_numpy(table_rgb_frame).float() / 255.0  # [0, 1]

# 2. Preprocessor å¤„ç†
policy_inputs = preprocessor(policy_inputs)
# observation.image: [0, 1] (ä¸å˜)
# observation.state: å½’ä¸€åŒ–å

# 3. Policy æ¨ç†
with torch.no_grad():
    action = policy.select_action(policy_inputs)  # å½’ä¸€åŒ–çš„ action

# 4. Postprocessor åå½’ä¸€åŒ–
action = postprocessor(action)  # åå½’ä¸€åŒ–åˆ°åŸå§‹èŒƒå›´
```

---

## ä¸ºä»€ä¹ˆXYZå¯è§†åŒ–ä¸­å›¾åƒç»è¿‡å½’ä¸€åŒ–

### è®­ç»ƒæ—¶çš„XYZå¯è§†åŒ–

**ä½ç½®**: `src/rev2fwd_il/train/lerobot_train_with_viz.py` â†’ `extract_xyz_visualization_data()`

```python
def extract_xyz_visualization_data(
    raw_batch: dict[str, torch.Tensor],
    processed_batch: dict[str, torch.Tensor],
    ...
) -> dict:
    # ä» processed_batch æå–å›¾åƒ (å·²ç»å½’ä¸€åŒ–åˆ° [0, 1])
    if "observation.image" in processed_batch:
        img = processed_batch["observation.image"]  # (B, n_obs_steps, C, H, W) [0, 1]
        if img.dim() == 5:
            img_np = img[0, -1].detach().cpu().numpy()  # (C, H, W) [0, 1]
        else:
            img_np = img[0].detach().cpu().numpy()  # (C, H, W) [0, 1]
        
        # è½¬æ¢ä¸º HWC æ ¼å¼å¹¶åå½’ä¸€åŒ–åˆ° [0, 255]
        img_np = np.transpose(img_np, (1, 2, 0))  # CHW -> HWC
        img_np = (img_np * 255).clip(0, 255).astype(np.uint8)  # [0, 1] -> [0, 255]
        viz_data["table_image"] = img_np
```

**å…³é”®ç‚¹**:
- XYZ å¯è§†åŒ–ä½¿ç”¨çš„æ˜¯ **`processed_batch`** ä¸­çš„å›¾åƒ
- `processed_batch` æ˜¯ç»è¿‡ Dataset åŠ è½½åçš„æ•°æ®ï¼Œå›¾åƒå·²ç»å½’ä¸€åŒ–åˆ° [0, 1]
- ä¸ºäº†æ˜¾ç¤ºï¼Œéœ€è¦ **åå½’ä¸€åŒ–å› [0, 255]**: `* 255`

### æ¨ç†æ—¶çš„XYZå¯è§†åŒ–

**ä½ç½®**: `scripts/41_test_A_diffusion_visualize.py` â†’ `run_episode()`

```python
# æ¨ç†æ—¶ç›´æ¥ä½¿ç”¨åŸå§‹çš„ uint8 å›¾åƒ
table_rgb_frame = table_rgb_np[0]  # (H, W, 3) uint8 [0, 255]

if xyz_visualizer is not None:
    xyz_visualizer.add_frame(
        ...
        table_image=table_rgb_frame,  # ç›´æ¥ä½¿ç”¨ uint8 [0, 255]
        ...
    )
```

**å…³é”®ç‚¹**:
- æ¨ç†æ—¶çš„ XYZ å¯è§†åŒ–ä½¿ç”¨çš„æ˜¯ **åŸå§‹çš„ uint8 å›¾åƒ**
- **ä¸éœ€è¦åå½’ä¸€åŒ–**ï¼Œå› ä¸ºä»ç›¸æœºè·å–çš„å°±æ˜¯ uint8 æ ¼å¼

### ä¸ºä»€ä¹ˆè®­ç»ƒæ—¶çš„å¯è§†åŒ–å›¾åƒ"ç»è¿‡å½’ä¸€åŒ–"

**ç­”æ¡ˆ**: è¿™æ˜¯ä¸€ä¸ª **æœ¯è¯­æ··æ·†**

- è®­ç»ƒæ—¶çš„å¯è§†åŒ–å›¾åƒç¡®å®æ¥è‡ª **å½’ä¸€åŒ–åçš„æ•°æ®** ([0, 1])
- ä½†åœ¨æ˜¾ç¤ºå‰ä¼š **åå½’ä¸€åŒ–å› [0, 255]**
- æ‰€ä»¥æœ€ç»ˆæ˜¾ç¤ºçš„å›¾åƒå’Œæ¨ç†æ—¶çš„å›¾åƒ **åœ¨è§†è§‰ä¸Šæ˜¯ä¸€æ ·çš„**

**çœŸæ­£çš„åŒºåˆ«**:
- è®­ç»ƒæ—¶: `Dataset [0, 1]` â†’ `åå½’ä¸€åŒ– [0, 255]` â†’ æ˜¾ç¤º
- æ¨ç†æ—¶: `ç›¸æœº [0, 255]` â†’ ç›´æ¥æ˜¾ç¤º

---

## Lossè®¡ç®—ä¸­æ˜¯å¦åŒ…å«å›¾åƒLoss

### Diffusion Policy çš„ Loss è®¡ç®—

**ä½ç½®**: LeRobot çš„ `DiffusionPolicy.forward()`

```python
def forward(self, batch: dict[str, torch.Tensor]) -> tuple[torch.Tensor, dict]:
    """
    è®¡ç®— Diffusion Policy çš„è®­ç»ƒ loss
    
    Args:
        batch: åŒ…å« observation.image, observation.state, action
        
    Returns:
        loss: MSE loss between predicted and ground truth actions
        output_dict: åŒ…å«ä¸­é—´ç»“æœçš„å­—å…¸
    """
    # 1. æå–è¾“å…¥
    obs_image = batch["observation.image"]  # (B, n_obs_steps, C, H, W) [0, 1]
    obs_state = batch["observation.state"]  # (B, n_obs_steps, state_dim) å½’ä¸€åŒ–å
    action_gt = batch["action"]  # (B, horizon, action_dim) å½’ä¸€åŒ–å
    
    # 2. ç¼–ç è§‚æµ‹ (å›¾åƒ + çŠ¶æ€)
    obs_features = self.encode_observation(obs_image, obs_state)
    
    # 3. æ·»åŠ å™ªå£°åˆ° ground truth action (diffusion forward process)
    noise = torch.randn_like(action_gt)
    timesteps = torch.randint(0, self.num_train_timesteps, (B,))
    noisy_action = self.noise_scheduler.add_noise(action_gt, noise, timesteps)
    
    # 4. é¢„æµ‹å™ªå£° (diffusion model)
    noise_pred = self.unet(
        sample=noisy_action,
        timestep=timesteps,
        encoder_hidden_states=obs_features,
    )
    
    # 5. è®¡ç®— MSE loss (åªé’ˆå¯¹ action)
    loss = F.mse_loss(noise_pred, noise)
    
    return loss, {"noise_pred": noise_pred, "noise_gt": noise}
```

**å…³é”®ç‚¹**:
1. **å›¾åƒåªä½œä¸ºè¾“å…¥ç‰¹å¾**ï¼Œé€šè¿‡ Vision Backbone (ResNet18) ç¼–ç 
2. **Loss åªè®¡ç®— action çš„é¢„æµ‹è¯¯å·®**ï¼Œä¸åŒ…å«å›¾åƒé‡å»º loss
3. è¿™æ˜¯ä¸€ä¸ª **æ¡ä»¶ç”Ÿæˆæ¨¡å‹**: ç»™å®š (image, state)ï¼Œé¢„æµ‹ action
4. å›¾åƒé€šè¿‡ Vision Backbone æå–ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾ç”¨äºæŒ‡å¯¼ action ç”Ÿæˆ

### ä¸ºä»€ä¹ˆä¸è®¡ç®—å›¾åƒ Loss?

**åŸå› **:
1. **ä»»åŠ¡ç›®æ ‡**: å­¦ä¹ ä»è§‚æµ‹ (image + state) åˆ°åŠ¨ä½œ (action) çš„æ˜ å°„
2. **ä¸æ˜¯å›¾åƒç”Ÿæˆä»»åŠ¡**: ä¸éœ€è¦é‡å»ºæˆ–ç”Ÿæˆå›¾åƒ
3. **å›¾åƒæ˜¯æ¡ä»¶**: å›¾åƒä½œä¸ºæ¡ä»¶è¾“å…¥ï¼Œå¸®åŠ©é¢„æµ‹æ›´å‡†ç¡®çš„ action
4. **è®¡ç®—æ•ˆç‡**: å›¾åƒ loss è®¡ç®—æˆæœ¬é«˜ï¼Œä¸”å¯¹ä»»åŠ¡æ— ç›Š

### Observation Image GT æ˜¯å¦ç»è¿‡å½’ä¸€åŒ–?

**ç­”æ¡ˆ**: **æ˜¯çš„**

```python
# è®­ç»ƒæ—¶çš„ ground truth
batch["observation.image"]  # (B, n_obs_steps, C, H, W) float32 [0, 1]
batch["observation.state"]  # (B, n_obs_steps, state_dim) å½’ä¸€åŒ–å (mean-std)
batch["action"]  # (B, horizon, action_dim) å½’ä¸€åŒ–å (mean-std)
```

**å½’ä¸€åŒ–æµç¨‹**:
1. **å›¾åƒ**: Dataset åŠ è½½æ—¶å½’ä¸€åŒ–åˆ° [0, 1]
2. **çŠ¶æ€**: Preprocessor ä½¿ç”¨ mean-std å½’ä¸€åŒ–
3. **åŠ¨ä½œ**: Preprocessor ä½¿ç”¨ mean-std å½’ä¸€åŒ–

---

## ğŸ› é—®é¢˜è¯Šæ–­ä¸ä¿®å¤

### é—®é¢˜æ ¹æº

é€šè¿‡è¯Šæ–­è„šæœ¬ `scripts/debug_image_normalization.py` å‘ç°ï¼š

1. **è®­ç»ƒé…ç½®é”™è¯¯**:
   ```json
   "normalization_mapping": {
       "VISUAL": "MEAN_STD",  // â† ä½¿ç”¨äº† ImageNet å½’ä¸€åŒ–ï¼
       "STATE": "MIN_MAX",
       "ACTION": "MIN_MAX"
   }
   ```

2. **ImageNet å½’ä¸€åŒ–å‚æ•°**:
   - mean: `[0.485, 0.456, 0.406]` (RGB)
   - std: `[0.229, 0.224, 0.225]` (RGB)

3. **å¯è§†åŒ–ä»£ç é—®é¢˜**:
   - ä» `processed_batch` æå–å›¾åƒï¼ˆå·²ç» ImageNet å½’ä¸€åŒ–ï¼‰
   - ç›´æ¥ `* 255` è½¬æ¢ä¸º uint8ï¼Œ**æ²¡æœ‰åå½’ä¸€åŒ–**
   - å¯¼è‡´å›¾åƒè¿‡æ›å’Œåè‰²

### ä¿®å¤æ–¹æ¡ˆ

**ä¿®å¤ä½ç½® 1**: `src/rev2fwd_il/train/lerobot_train_with_viz.py`

åœ¨å¯è§†åŒ–å‰æ·»åŠ åå½’ä¸€åŒ–ï¼š

```python
# IMPORTANT: Reverse ImageNet normalization before visualization
# Images are normalized with: (img - mean) / std
# We need to reverse: img = normalized * std + mean
imagenet_mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)
imagenet_std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)
img_np = img_np * imagenet_std + imagenet_mean  # Reverse normalization

# Then convert to uint8
img_np = np.transpose(img_np, (1, 2, 0))  # CHW -> HWC
img_np = (img_np * 255).clip(0, 255).astype(np.uint8)
```

**ä¿®å¤ä½ç½® 2**: `scripts/41_test_A_diffusion_visualize.py`

æ·»åŠ  debug è¾“å‡ºéªŒè¯å½’ä¸€åŒ–ï¼š

```python
# DEBUG: Print image stats before/after preprocessing
if t == 0:
    print(f"[DEBUG] Step 0: Image BEFORE preprocessing:")
    print(f"  Range: [{table_rgb_chw.min():.4f}, {table_rgb_chw.max():.4f}]")
    print(f"  Mean per channel: R={...}, G={...}, B={...}")
    
    # After preprocessing
    print(f"[DEBUG] Step 0: After preprocessing (ImageNet normalized):")
    print(f"  Range: [{policy_inputs['observation.image'].min():.4f}, ...]")
    print(f"  Expected after ImageNet norm: mean~0, std~1 per channel")
```

### éªŒè¯ç»“æœ

è¿è¡Œ `scripts/test_image_normalization_fix.py`:

```
1. Original image (mid-gray):
   Mean per channel: R=0.5000, G=0.5000, B=0.5000

2. After ImageNet normalization:
   Mean per channel: R=0.0655, G=0.1964, B=0.4178

3. OLD (WRONG) visualization (normalized * 255):
   Mean per channel: R=16.0, G=50.0, B=106.0
   âš ï¸  This gives wrong colors! (should be ~127 for mid-gray)

4. NEW (CORRECT) visualization (reverse norm, then * 255):
   Mean per channel: R=127.0, G=127.0, B=127.0
   âœ“ Correct! Mid-gray should be ~127

5. Verification:
   âœ“ PASS: Visualization is correct!
```

---

## æ€»ç»“

### å›¾åƒå½’ä¸€åŒ–å¯¹æ¯”è¡¨ï¼ˆæ›´æ–°ï¼‰

| é˜¶æ®µ | è®­ç»ƒ | æ¨ç† |
|------|------|------|
| **åŸå§‹æ ¼å¼** | uint8 [0, 255] (NPZ) | uint8 [0, 255] (ç›¸æœº) |
| **Dataset åŠ è½½** | float32 [0, 1] (è‡ªåŠ¨) | N/A |
| **æ‰‹åŠ¨å½’ä¸€åŒ–** | N/A | float32 [0, 1] (`/ 255.0`) |
| **Preprocessor** | **ImageNet MEAN_STD** âš ï¸ | **ImageNet MEAN_STD** âš ï¸ |
| **è¾“å…¥ Policy** | ImageNet å½’ä¸€åŒ– (mean~0, std~1) | ImageNet å½’ä¸€åŒ– (mean~0, std~1) |
| **XYZ å¯è§†åŒ–** | **å ImageNet å½’ä¸€åŒ–** â†’ [0, 1] â†’ [0, 255] | ç›´æ¥ä½¿ç”¨ [0, 255] |

### å…³é”®ç»“è®ºï¼ˆæ›´æ–°ï¼‰

1. **è®­ç»ƒå’Œæ¨ç†éƒ½ä½¿ç”¨ ImageNet å½’ä¸€åŒ–** âš ï¸
   - Dataset åŠ è½½: uint8 â†’ float32 [0, 1]
   - Preprocessor: [0, 1] â†’ ImageNet å½’ä¸€åŒ– (mean~0, std~1)
   - å½’ä¸€åŒ–æ–¹å¼: `(img - mean) / std`
   - mean: `[0.485, 0.456, 0.406]`, std: `[0.229, 0.224, 0.225]`

2. **å¯è§†åŒ–é—®é¢˜å·²ä¿®å¤** âœ…
   - **æ—§ä»£ç **: ç›´æ¥ `normalized_img * 255` â†’ è¿‡æ›å’Œåè‰²
   - **æ–°ä»£ç **: å…ˆåå½’ä¸€åŒ– `img = normalized * std + mean`ï¼Œå† `* 255`
   - ä¿®å¤ä½ç½®: `lerobot_train_with_viz.py` (4å¤„)

3. **Loss è®¡ç®—ä¸åŒ…å«å›¾åƒ Loss**
   - åªè®¡ç®— action çš„ MSE loss
   - å›¾åƒé€šè¿‡ Vision Backbone æå–ç‰¹å¾ï¼Œä½œä¸ºæ¡ä»¶è¾“å…¥

4. **Observation Image GT ç»è¿‡ ImageNet å½’ä¸€åŒ–**
   - å›¾åƒ: ImageNet MEAN_STD å½’ä¸€åŒ–
   - çŠ¶æ€: MIN_MAX å½’ä¸€åŒ–
   - åŠ¨ä½œ: MIN_MAX å½’ä¸€åŒ–

5. **æ¨ç†æ—¶çš„å½’ä¸€åŒ–æµç¨‹**
   - ç›¸æœº â†’ uint8 [0, 255]
   - æ‰‹åŠ¨å½’ä¸€åŒ– â†’ float32 [0, 1]
   - Preprocessor â†’ ImageNet å½’ä¸€åŒ– (mean~0, std~1)
   - ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ âœ…

### é—®é¢˜è¯Šæ–­ä¸ä¿®å¤

#### é—®é¢˜è¡¨ç°
- XYZ å¯è§†åŒ–å›¾åƒè¿‡æ›
- é¢œè‰²åç§»ï¼ˆåè“è‰²ï¼‰
- å¯¹æ¯”åº¦å¼‚å¸¸

#### æ ¹æœ¬åŸå› 
```python
# é”™è¯¯çš„å¯è§†åŒ–ä»£ç 
img_np = processed_batch["observation.image"][0, -1]  # ImageNet å½’ä¸€åŒ–å
img_np = (img_np * 255).astype(np.uint8)  # âŒ ç›´æ¥ * 255ï¼Œæ²¡æœ‰åå½’ä¸€åŒ–
```

å¯¹äº mid-gray (0.5, 0.5, 0.5):
- ImageNet å½’ä¸€åŒ–å: (0.066, 0.196, 0.418)
- é”™è¯¯å¯è§†åŒ–: (16, 50, 106) â† åè“è‰²ï¼Œè¿‡æš—
- æ­£ç¡®åº”è¯¥æ˜¯: (127, 127, 127)

#### ä¿®å¤æ–¹æ³•
```python
# æ­£ç¡®çš„å¯è§†åŒ–ä»£ç 
img_np = processed_batch["observation.image"][0, -1]  # ImageNet å½’ä¸€åŒ–å

# âœ… å…ˆåå½’ä¸€åŒ–
imagenet_mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)
imagenet_std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)
img_np = img_np * imagenet_std + imagenet_mean  # åå½’ä¸€åŒ–åˆ° [0, 1]

# ç„¶åè½¬æ¢ä¸º uint8
img_np = np.transpose(img_np, (1, 2, 0))
img_np = (img_np * 255).clip(0, 255).astype(np.uint8)
```

### å¸¸è§è¯¯è§£æ¾„æ¸…ï¼ˆæ›´æ–°ï¼‰

âŒ **è¯¯è§£**: å›¾åƒå½’ä¸€åŒ–æ¨¡å¼æ˜¯ NONE
âœ… **äº‹å®**: å›¾åƒå½’ä¸€åŒ–æ¨¡å¼æ˜¯ **MEAN_STD (ImageNet)**ï¼Œåœ¨ config.json ä¸­æ˜ç¡®å®šä¹‰

âŒ **è¯¯è§£**: æ¨ç†æ—¶æ²¡æœ‰å½’ä¸€åŒ–å›¾åƒ
âœ… **äº‹å®**: æ¨ç†æ—¶ Preprocessor ä¼šåº”ç”¨ ImageNet å½’ä¸€åŒ–ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´

âŒ **è¯¯è§£**: å¯è§†åŒ–å›¾åƒè¿‡æ›æ˜¯æ•°æ®é—®é¢˜
âœ… **äº‹å®**: æ˜¯å¯è§†åŒ–ä»£ç é—®é¢˜ï¼Œæ²¡æœ‰åå½’ä¸€åŒ–å°±ç›´æ¥ * 255

âŒ **è¯¯è§£**: è®­ç»ƒæ—¶è®¡ç®—å›¾åƒ loss
âœ… **äº‹å®**: åªè®¡ç®— action çš„ MSE lossï¼Œå›¾åƒåªä½œä¸ºè¾“å…¥ç‰¹å¾

---

## ä»£ç ä½ç½®ç´¢å¼•

### è®­ç»ƒç›¸å…³
- æ•°æ®è½¬æ¢: `scripts/31_train_A_diffusion.py:convert_npz_to_lerobot_format()`
- Preprocessor é…ç½®: `src/rev2fwd_il/train/lerobot_train_with_viz.py:train_with_xyz_visualization()`
- XYZ å¯è§†åŒ–: `src/rev2fwd_il/train/lerobot_train_with_viz.py:extract_xyz_visualization_data()`

### æ¨ç†ç›¸å…³
- å›¾åƒè·å–å’Œå½’ä¸€åŒ–: `scripts/41_test_A_diffusion_visualize.py:run_episode()` line 638
- Preprocessor åŠ è½½: `scripts/41_test_A_diffusion_visualize.py:load_diffusion_policy()`
- XYZ å¯è§†åŒ–: `scripts/41_test_A_diffusion_visualize.py:run_episode()` (ç›´æ¥ä½¿ç”¨åŸå§‹å›¾åƒ)

### LeRobot å†…éƒ¨
- Dataset åŠ è½½: LeRobot çš„ `LeRobotDataset.__getitem__()`
- Policy forward: LeRobot çš„ `DiffusionPolicy.forward()`
- Preprocessor: LeRobot çš„ `make_pre_post_processors()`
