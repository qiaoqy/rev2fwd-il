#!/usr/bin/env python3
"""
8_analyze_rollout_record.py - Analyze iterative training performance from rollout records.

This script analyzes the rollout_record.json file generated by the iterative training
pipeline and creates visualizations to understand finetuning effectiveness.

Key Metrics:
-----------
1. Average Steps per Successful Task (ä¸»è¦æŒ‡æ ‡):
   - è¡¡é‡æˆåŠŸå®Œæˆä¸€ä¸ªä»»åŠ¡å¹³å‡éœ€è¦çš„æ­¥æ•°
   - è®¡ç®—æ–¹å¼: total_success_steps / num_completed_tasks
   - ä»»åŠ¡å¤±è´¥å·²ä½“çŽ°åœ¨ num_completed_tasks ä¸­ï¼ˆå¤±è´¥è¶Šå¤šï¼Œå®Œæˆæ•°è¶Šå°‘ï¼‰
   - æ­¥æ•°è¶Šå°‘ = æ¨¡åž‹æ‰§è¡Œæ•ˆçŽ‡è¶Šé«˜
   - è¶Šä½Žè¶Šå¥½

2. Task-specific Success Steps:
   - åˆ†åˆ«è¿½è¸ª Task A å’Œ Task B çš„å¹³å‡æˆåŠŸæ­¥æ•°
   - æ­¥æ•°è¶Šå°‘ = æ‰§è¡Œæ•ˆçŽ‡è¶Šé«˜

3. Consecutive Successes:
   - è¿žç»­æˆåŠŸçš„ Aâ†’B å¾ªçŽ¯æ¬¡æ•°
   - è¡¡é‡æ¨¡åž‹ç¨³å®šæ€§

Usage:
------
    python scripts/scripts_pick_place/8_analyze_rollout_record.py
    python scripts/scripts_pick_place/8_analyze_rollout_record.py --record data/rollout_record.json
    python scripts/scripts_pick_place/8_analyze_rollout_record.py --retry_penalty 400 --out data/analysis
"""

import argparse
import json
from pathlib import Path
from datetime import datetime
import numpy as np

# Try to import matplotlib, handle if not available
try:
    import matplotlib.pyplot as plt
    import matplotlib.patches as mpatches
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    print("Warning: matplotlib not installed. Will save data but skip plotting.")


def load_rollout_record(record_path: str) -> dict:
    """Load rollout record JSON file."""
    with open(record_path, 'r') as f:
        return json.load(f)


def compute_effort_per_task(
    iteration_data: dict,
    retry_penalty: float = 400.0,
    step_cost: float = 1.0,
    horizon: int = 400
) -> dict:
    """
    Compute the average effort (cost) per completed task for an iteration.
    
    Effort = (retry_attempts - 1) * retry_penalty + total_steps_for_all_tasks
    Average Effort = Effort / num_completed_tasks
    
    Args:
        iteration_data: Single iteration entry from rollout record
        retry_penalty: Cost penalty for each failed retry attempt
        step_cost: Cost per simulation step (default 1.0)
        horizon: Max steps per task attempt (used for failed tasks)
    
    Returns:
        dict with computed metrics
    """
    retry_attempt = iteration_data.get('retry_attempt', 1)
    rollout_results = iteration_data.get('rollout_results', {})
    summary = rollout_results.get('summary', {})
    
    # Get episode details
    episodes_A = rollout_results.get('episodes_A_details', [])
    episodes_B = rollout_results.get('episodes_B_details', [])
    
    # Count successes and compute steps
    success_count_A = 0
    success_count_B = 0
    total_steps_A = 0  # Only count successful task steps
    total_steps_B = 0
    failed_steps = 0  # Steps spent on failed attempts
    
    success_steps_A = []
    success_steps_B = []
    
    for ep in episodes_A:
        if ep.get('success'):
            success_count_A += 1
            steps = ep.get('success_step', ep.get('total_steps', horizon))
            total_steps_A += steps
            success_steps_A.append(steps)
        else:
            # Failed attempt costs horizon steps
            failed_steps += ep.get('total_steps', horizon)
    
    for ep in episodes_B:
        if ep.get('success'):
            success_count_B += 1
            steps = ep.get('success_step', ep.get('total_steps', horizon))
            total_steps_B += steps
            success_steps_B.append(steps)
        else:
            # Failed attempt costs horizon steps
            failed_steps += ep.get('total_steps', horizon)
    
    total_completed = success_count_A + success_count_B
    
    # Compute effort
    # Retry penalty: only applied when there are actual retries (retry_attempt > 1)
    # retry_attempt=1 means first attempt, no retry occurred
    retry_cost = max(0, retry_attempt - 1) * retry_penalty
    
    # Total effort = only successful task steps
    # Note: failed_steps is NOT included because task failure is already reflected
    # in num_completed_tasks (fewer completions = worse performance)
    total_effort = total_steps_A + total_steps_B
    
    # Average effort per completed task
    if total_completed > 0:
        avg_effort = total_effort / total_completed
    else:
        avg_effort = float('inf')  # No tasks completed
    
    return {
        'iteration': iteration_data.get('iteration'),
        'retry_attempt': retry_attempt,
        'retry_cost': retry_cost,
        'success_count_A': success_count_A,
        'success_count_B': success_count_B,
        'total_completed': total_completed,
        'total_steps_A': total_steps_A,
        'total_steps_B': total_steps_B,
        'failed_steps': failed_steps,
        'total_effort': total_effort,
        'avg_effort_per_task': avg_effort,
        'consecutive_successes': summary.get('consecutive_successes', 0),
        'success_steps_A': success_steps_A,
        'success_steps_B': success_steps_B,
        'avg_success_step_A': np.mean(success_steps_A) if success_steps_A else None,
        'avg_success_step_B': np.mean(success_steps_B) if success_steps_B else None,
        'training_step_A': iteration_data.get('checkpoint_info', {}).get('policy_A_training_step'),
        'training_step_B': iteration_data.get('checkpoint_info', {}).get('policy_B_training_step'),
    }


def analyze_rollout_record(record: dict, retry_penalty: float = 400.0) -> list:
    """Analyze all iterations in the rollout record."""
    iterations = record.get('iterations', [])
    horizon = record.get('config', {}).get('horizon', 400)
    
    results = []
    for iter_data in iterations:
        metrics = compute_effort_per_task(iter_data, retry_penalty=retry_penalty, horizon=horizon)
        results.append(metrics)
    
    return results


def plot_effort_analysis(
    analysis_results: list,
    output_dir: str,
    retry_penalty: float = 400.0
):
    """Create visualization plots for the analysis."""
    if not HAS_MATPLOTLIB:
        print("Skipping plots (matplotlib not available)")
        return
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # Extract data
    iterations = [r['iteration'] for r in analysis_results]
    training_steps = [r['training_step_A'] for r in analysis_results]
    avg_efforts = [r['avg_effort_per_task'] for r in analysis_results]
    consecutive_successes = [r['consecutive_successes'] for r in analysis_results]
    total_completed = [r['total_completed'] for r in analysis_results]
    
    # Handle infinite values for plotting
    max_finite = max([e for e in avg_efforts if e != float('inf')], default=500)
    avg_efforts_plot = [e if e != float('inf') else max_finite * 1.2 for e in avg_efforts]
    
    # =========================================================================
    # Figure 1: Main Effort Analysis (Average Effort per Task)
    # =========================================================================
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    fig.suptitle('Iterative Training Performance Analysis', fontsize=14, fontweight='bold')
    
    # Plot 1: Average Effort per Task vs Iteration
    ax1 = axes[0, 0]
    colors = ['red' if e == float('inf') else 'steelblue' for e in avg_efforts]
    bars = ax1.bar(iterations, avg_efforts_plot, color=colors, alpha=0.7, edgecolor='black')
    ax1.axhline(y=retry_penalty, color='orange', linestyle='--', label=f'Retry Penalty ({retry_penalty})')
    ax1.set_xlabel('Iteration', fontsize=11)
    ax1.set_ylabel('Average Effort per Task (steps)', fontsize=11)
    ax1.set_title('Average Effort per Task\n(Lower is Better)', fontsize=12)
    ax1.legend(loc='upper right')
    ax1.grid(axis='y', alpha=0.3)
    
    # Add annotation for infinite values
    for i, (x, y, e) in enumerate(zip(iterations, avg_efforts_plot, avg_efforts)):
        if e == float('inf'):
            ax1.annotate('âˆž\n(no success)', (x, y), ha='center', va='bottom', fontsize=8, color='red')
    
    # Plot 2: Total Completed Tasks per Iteration
    ax2 = axes[0, 1]
    success_A = [r['success_count_A'] for r in analysis_results]
    success_B = [r['success_count_B'] for r in analysis_results]
    
    x = np.arange(len(iterations))
    width = 0.35
    ax2.bar(x - width/2, success_A, width, label='Task A', color='coral', alpha=0.8)
    ax2.bar(x + width/2, success_B, width, label='Task B', color='teal', alpha=0.8)
    ax2.set_xlabel('Iteration', fontsize=11)
    ax2.set_ylabel('Success Count', fontsize=11)
    ax2.set_title('Successful Task Completions per Iteration', fontsize=12)
    ax2.set_xticks(x)
    ax2.set_xticklabels(iterations)
    ax2.legend()
    ax2.grid(axis='y', alpha=0.3)
    
    # Plot 3: Consecutive Successes (Aâ†’B cycles)
    ax3 = axes[1, 0]
    ax3.plot(iterations, consecutive_successes, 'o-', color='green', markersize=8, linewidth=2)
    ax3.fill_between(iterations, consecutive_successes, alpha=0.3, color='green')
    ax3.set_xlabel('Iteration', fontsize=11)
    ax3.set_ylabel('Consecutive Aâ†’B Cycles', fontsize=11)
    ax3.set_title('Consecutive Successes\n(Higher is Better - Shows Stability)', fontsize=12)
    ax3.grid(alpha=0.3)
    ax3.set_ylim(bottom=0)
    
    # Plot 4: Average Success Steps by Task
    ax4 = axes[1, 1]
    avg_steps_A = [r['avg_success_step_A'] for r in analysis_results]
    avg_steps_B = [r['avg_success_step_B'] for r in analysis_results]
    
    # Filter out None values for plotting
    valid_A = [(i, s) for i, s in zip(iterations, avg_steps_A) if s is not None]
    valid_B = [(i, s) for i, s in zip(iterations, avg_steps_B) if s is not None]
    
    if valid_A:
        ax4.plot([x[0] for x in valid_A], [x[1] for x in valid_A], 'o-', 
                 color='coral', label='Task A', markersize=6, linewidth=2)
    if valid_B:
        ax4.plot([x[0] for x in valid_B], [x[1] for x in valid_B], 's-', 
                 color='teal', label='Task B', markersize=6, linewidth=2)
    
    ax4.set_xlabel('Iteration', fontsize=11)
    ax4.set_ylabel('Average Steps to Success', fontsize=11)
    ax4.set_title('Average Steps per Successful Task\n(Lower is Better - Shows Efficiency)', fontsize=12)
    ax4.legend()
    ax4.grid(alpha=0.3)
    
    plt.tight_layout()
    
    # Save figure
    output_file = output_path / 'rollout_analysis.png'
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"Saved: {output_file}")
    plt.close()
    
    # =========================================================================
    # Figure 2: Effort Breakdown
    # =========================================================================
    fig2, ax = plt.subplots(figsize=(12, 6))
    
    retry_costs = [r['retry_cost'] for r in analysis_results]
    failed_steps = [r['failed_steps'] for r in analysis_results]
    success_steps_A = [r['total_steps_A'] for r in analysis_results]
    success_steps_B = [r['total_steps_B'] for r in analysis_results]
    
    x = np.arange(len(iterations))
    width = 0.6
    
    # Stacked bar chart (only showing successful task steps)
    p1 = ax.bar(x, success_steps_A, width, label='Success Steps (A)', color='coral', alpha=0.7)
    p2 = ax.bar(x, success_steps_B, width, bottom=np.array(success_steps_A),
                label='Success Steps (B)', color='teal', alpha=0.7)
    
    ax.set_xlabel('Iteration', fontsize=11)
    ax.set_ylabel('Total Effort (steps)', fontsize=11)
    ax.set_title('Effort Breakdown by Iteration\n(Shows where effort is spent)', fontsize=12, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(iterations)
    ax.legend(loc='upper right')
    ax.grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    output_file2 = output_path / 'effort_breakdown.png'
    plt.savefig(output_file2, dpi=150, bbox_inches='tight')
    print(f"Saved: {output_file2}")
    plt.close()
    
    # =========================================================================
    # Figure 3: Training Progress (Steps vs Performance)
    # =========================================================================
    fig3, ax = plt.subplots(figsize=(12, 6))
    
    # Use training steps as x-axis if available
    if all(s is not None for s in training_steps):
        x_axis = [s/1000 for s in training_steps]  # Convert to thousands
        has_training_steps = True
    else:
        x_axis = iterations
        has_training_steps = False
    
    # Plot effort with trend line
    finite_mask = [e != float('inf') for e in avg_efforts]
    finite_x = [x for x, m in zip(x_axis, finite_mask) if m]
    finite_y = [e for e, m in zip(avg_efforts, finite_mask) if m]
    
    # Plot data points with line connecting them
    ax.plot(x_axis, avg_efforts_plot, 'o-', color='steelblue', markersize=8, 
            linewidth=1.5, alpha=0.8, label='Avg Steps per Task', zorder=3,
            markeredgecolor='black', markeredgewidth=1)
    
    # Add trend line if enough data
    if len(finite_x) >= 3:
        z = np.polyfit(finite_x, finite_y, 1)
        p = np.poly1d(z)
        x_trend = np.linspace(min(finite_x), max(finite_x), 100)
        ax.plot(x_trend, p(x_trend), '--', color='red', alpha=0.7, 
                label=f'Linear Trend (slope={z[0]:.2f})', linewidth=2)
    
    # Always show legend
    ax.legend(loc='upper right', fontsize=10)
    
    # Set up dual x-axis labels (training steps + iteration)
    if has_training_steps:
        ax.set_xlabel('Training Steps (k)', fontsize=11)
        ax.set_xticks(x_axis)
        # Create dual labels: training steps on top, iteration below
        x_labels = [f'{int(s)}k\n(iter {i})' for s, i in zip(x_axis, iterations)]
        ax.set_xticklabels(x_labels, fontsize=9)
    else:
        ax.set_xlabel('Iteration', fontsize=11)
    
    ax.set_ylabel('Average Steps per Successful Task', fontsize=11)
    ax.set_title('Training Progress: Execution Efficiency over Iterations\n(Lower = More Efficient)', fontsize=12, fontweight='bold')
    ax.grid(alpha=0.3)
    
    plt.tight_layout()
    output_file3 = output_path / 'training_progress.png'
    plt.savefig(output_file3, dpi=150, bbox_inches='tight')
    print(f"Saved: {output_file3}")
    plt.close()


def print_summary(analysis_results: list, record: dict):
    """Print a text summary of the analysis."""
    print("\n" + "="*70)
    print("ROLLOUT ANALYSIS SUMMARY")
    print("="*70)
    
    config = record.get('config', {})
    print(f"\nConfiguration:")
    print(f"  - Max iterations: {config.get('max_iterations')}")
    print(f"  - Steps per iteration: {config.get('steps_per_iter')}")
    print(f"  - Horizon: {config.get('horizon')}")
    print(f"  - Distance threshold: {config.get('distance_threshold')}")
    
    print(f"\nAnalyzed {len(analysis_results)} iterations")
    
    # Find best and worst iterations
    finite_results = [r for r in analysis_results if r['avg_effort_per_task'] != float('inf')]
    
    if finite_results:
        best = min(finite_results, key=lambda x: x['avg_effort_per_task'])
        worst = max(finite_results, key=lambda x: x['avg_effort_per_task'])
        
        print(f"\nBest Iteration: #{best['iteration']}")
        print(f"  - Average effort: {best['avg_effort_per_task']:.1f} steps/task")
        print(f"  - Completed tasks: {best['total_completed']} (A:{best['success_count_A']}, B:{best['success_count_B']})")
        print(f"  - Consecutive successes: {best['consecutive_successes']}")
        
        print(f"\nWorst Iteration (with completions): #{worst['iteration']}")
        print(f"  - Average effort: {worst['avg_effort_per_task']:.1f} steps/task")
        print(f"  - Completed tasks: {worst['total_completed']} (A:{worst['success_count_A']}, B:{worst['success_count_B']})")
    
    # Statistics
    efforts = [r['avg_effort_per_task'] for r in finite_results]
    if efforts:
        print(f"\nEffort Statistics (excluding failed iterations):")
        print(f"  - Mean: {np.mean(efforts):.1f}")
        print(f"  - Std: {np.std(efforts):.1f}")
        print(f"  - Min: {np.min(efforts):.1f}")
        print(f"  - Max: {np.max(efforts):.1f}")
    
    # Task-specific stats
    steps_A = [r['avg_success_step_A'] for r in analysis_results if r['avg_success_step_A'] is not None]
    steps_B = [r['avg_success_step_B'] for r in analysis_results if r['avg_success_step_B'] is not None]
    
    print(f"\nTask A Success Steps:")
    if steps_A:
        print(f"  - Mean: {np.mean(steps_A):.1f}")
        print(f"  - Range: [{np.min(steps_A):.0f}, {np.max(steps_A):.0f}]")
    else:
        print("  - No successful Task A completions")
    
    print(f"\nTask B Success Steps:")
    if steps_B:
        print(f"  - Mean: {np.mean(steps_B):.1f}")
        print(f"  - Range: [{np.min(steps_B):.0f}, {np.max(steps_B):.0f}]")
    else:
        print("  - No successful Task B completions")
    
    # Trend analysis
    if len(finite_results) >= 3:
        x = [r['iteration'] for r in finite_results]
        y = [r['avg_effort_per_task'] for r in finite_results]
        slope, _ = np.polyfit(x, y, 1)
        
        print(f"\nTrend Analysis:")
        if slope < -1:
            print(f"  ðŸ“‰ Improving (slope={slope:.2f}): Model is getting more efficient")
        elif slope > 1:
            print(f"  ðŸ“ˆ Degrading (slope={slope:.2f}): Model performance may be declining")
        else:
            print(f"  âž¡ï¸ Stable (slope={slope:.2f}): Performance is relatively constant")
    
    # Identify issues
    print(f"\nPotential Issues:")
    no_B_success = sum(1 for r in analysis_results if r['success_count_B'] == 0)
    if no_B_success > len(analysis_results) * 0.5:
        print(f"  âš ï¸ Task B rarely succeeds ({no_B_success}/{len(analysis_results)} iterations)")
        print(f"     Consider: More Task B training data, adjust distance threshold, check policy B")
    
    no_consecutive = sum(1 for r in analysis_results if r['consecutive_successes'] == 0)
    if no_consecutive > len(analysis_results) * 0.7:
        print(f"  âš ï¸ Low consecutive successes ({no_consecutive}/{len(analysis_results)} iterations with 0)")
        print(f"     The Aâ†’Bâ†’A loop often fails on first B attempt")
    
    print("\n" + "="*70)


def save_analysis_json(analysis_results: list, output_path: str):
    """Save detailed analysis results as JSON."""
    Path(output_path).mkdir(parents=True, exist_ok=True)
    
    output = {
        'generated_at': datetime.now().isoformat(),
        'summary': {
            'total_iterations': len(analysis_results),
            'iterations_with_completions': sum(1 for r in analysis_results if r['total_completed'] > 0),
        },
        'per_iteration': analysis_results,
    }
    
    # Compute aggregate stats
    finite_efforts = [r['avg_effort_per_task'] for r in analysis_results if r['avg_effort_per_task'] != float('inf')]
    if finite_efforts:
        output['summary']['mean_effort'] = float(np.mean(finite_efforts))
        output['summary']['std_effort'] = float(np.std(finite_efforts))
        output['summary']['min_effort'] = float(np.min(finite_efforts))
        output['summary']['max_effort'] = float(np.max(finite_efforts))
    
    # Convert inf to string for JSON
    for r in output['per_iteration']:
        if r['avg_effort_per_task'] == float('inf'):
            r['avg_effort_per_task'] = 'inf'
    
    output_file = Path(output_path) / 'analysis_results.json'
    with open(output_file, 'w') as f:
        json.dump(output, f, indent=2)
    print(f"Saved: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description='Analyze rollout record from iterative training',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/scripts_pick_place/8_analyze_rollout_record.py
  python scripts/scripts_pick_place/8_analyze_rollout_record.py --retry_penalty 500
  python scripts/scripts_pick_place/8_analyze_rollout_record.py --record data/rollout_record.json --out data/analysis
        """
    )
    parser.add_argument('--record', type=str, default='data/rollout_record.json',
                        help='Path to rollout_record.json file')
    parser.add_argument('--out', type=str, default='data/analysis',
                        help='Output directory for plots and analysis')
    parser.add_argument('--retry_penalty', type=float, default=400.0,
                        help='Penalty cost for each failed retry attempt (default: 400)')
    
    args = parser.parse_args()
    
    # Load data
    print(f"Loading rollout record: {args.record}")
    record = load_rollout_record(args.record)
    
    # Analyze
    print(f"Analyzing {len(record.get('iterations', []))} iterations...")
    analysis_results = analyze_rollout_record(record, retry_penalty=args.retry_penalty)
    
    # Print summary
    print_summary(analysis_results, record)
    
    # Save analysis JSON
    save_analysis_json(analysis_results, args.out)
    
    # Create plots
    print("\nGenerating plots...")
    plot_effort_analysis(analysis_results, args.out, retry_penalty=args.retry_penalty)
    
    print(f"\nDone! Results saved to: {args.out}/")


if __name__ == '__main__':
    main()
